{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neural_network as nn\n",
    "import LSTM\n",
    "import datapreprocessing as dp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interactions():\n",
    "    df_user_item_rating = pd.read_csv('cleaned_user_item_rating.csv')\n",
    "    df_user_item_rating = df_user_item_rating.drop(columns=['Unnamed: 0'])[:10000]\n",
    "    df_user_item_rating['reviewerID'] = pd.factorize(df_user_item_rating['reviewerID'])[0] + 1\n",
    "    df_user_item_rating['asin'] = pd.factorize(df_user_item_rating['asin'])[0] + 1\n",
    "    \n",
    "    # number of unique users\n",
    "    n_users = df_user_item_rating.reviewerID.unique().shape[0]\n",
    "    #print('{} unique users'.format(n_users))\n",
    "    \n",
    "    # number of unique items\n",
    "    n_items = df_user_item_rating.asin.unique().shape[0]\n",
    "    #print('{} unique items'.format(n_items))\n",
    "    \n",
    "    interactions = np.zeros((n_users, n_items))\n",
    "    for row in df_user_item_rating.itertuples():\n",
    "        interactions[row[1] - 1, row[2] - 1] = row[3]\n",
    "    return interactions, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions, n_items = get_interactions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the underwhelming performance of our matrix factorization model, we try a simple feedforward recommendation system instead. \n",
    "\n",
    "\n",
    "Input of this neural network is a pair of user and item represented by their IDs. Both user and item IDs first pass through an embedding layer. The output of the embedding layer, which are two embedding vectors, are then concatenated into one and passed into a linear network. \n",
    "\n",
    "\n",
    "Output of the linear network is one dimensional - representing the rating for the user-item pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit the same way as the matrix factorization model and uses the standard PyTorch approach of forward passing, computing the loss, backpropagating and updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(interactions, n=2):\n",
    "    \"\"\"\n",
    "    Split an interactions matrix into training and test sets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    interactions : np.ndarray\n",
    "    n : int (default=2)\n",
    "        Number of items to select / row to place into test.\n",
    "    Returns\n",
    "    -------\n",
    "    train : np.ndarray\n",
    "    test : np.ndarray\n",
    "    \"\"\"\n",
    "    test = np.zeros(interactions.shape)\n",
    "    train = interactions.copy()\n",
    "    for user in range(interactions.shape[0]):\n",
    "        if interactions[user, :].nonzero()[0].shape[0] > n:\n",
    "            test_interactions = np.random.choice(interactions[user, :].nonzero()[0],\n",
    "                                                 size=n,\n",
    "                                                 replace=False)\n",
    "            train[user, test_interactions] = 0.\n",
    "            test[user, test_interactions] = interactions[user, test_interactions]\n",
    "\n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions_train_test_split():\n",
    "    interactions, _ = get_interactions()\n",
    "    train, test = train_test_split(interactions)\n",
    "    train = sparse.coo_matrix(train)\n",
    "    test = sparse.coo_matrix(test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = interactions_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize into [0,1]\n",
    "def normalize(ratings):\n",
    "    ratings = ratings.copy()\n",
    "    max_ratings = ratings.max()\n",
    "    ratings = ratings*1.0/max_ratings\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.data = normalize(training_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_rating(training_data):\n",
    "    n_users = training_data.shape[0]\n",
    "    sequence = []\n",
    "    ratings = []\n",
    "    for users in range(0, n_users):\n",
    "        sequence.append(np.nonzero(training_data.toarray()[users])[0])\n",
    "        ratings.append(training_data.toarray()[users][[np.nonzero(training_data.toarray()[users])[0]]])\n",
    "    return sequence, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_list, ratings_list = seq_rating(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, 0.027963126078248024\n",
      "Train Epoch: 2, 0.02094120904803276\n",
      "Train Epoch: 3, 0.019772373139858246\n",
      "Train Epoch: 4, 0.019391752779483795\n",
      "Train Epoch: 5, 0.01922580599784851\n",
      "Train Epoch: 6, 0.01913793757557869\n",
      "Train Epoch: 7, 0.01908610761165619\n",
      "Train Epoch: 8, 0.019056079909205437\n",
      "Train Epoch: 9, 0.01902804896235466\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "n_output = 1\n",
    "\n",
    "# add one to represent padding when there is not enough history\n",
    "model = LSTM.LSTMRating(embedding_dim, hidden_dim, n_items + 1, n_output)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#if os.path.isfile('LSTM.pt'):\n",
    "#    model = torch.load('LSTM.pt')\n",
    "\n",
    "def train(epoch, sequence_list, ratings_list):\n",
    "    loss_hist = []\n",
    "    loss_average_list = []\n",
    "    t = 0\n",
    "    for sequence, target_ratings in zip(sequence_list, ratings_list):\n",
    "        t = t + 1\n",
    "        model.zero_grad()\n",
    "        # initialize hidden layers\n",
    "        model.hidden = model.init_hidden()\n",
    "        # convert sequence to PyTorch variables\n",
    "        sequence_var = torch.LongTensor(sequence.astype('int64'))\n",
    "        # forward pass\n",
    "        ratings_scores = model(sequence_var).flatten()\n",
    "        target_ratings_var = torch.FloatTensor(target_ratings.astype('float32'))\n",
    "        # compute loss\n",
    "        loss = loss_fn(ratings_scores, target_ratings_var)\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # print\n",
    "        if t % 7000 == 0:\n",
    "            #running_loss = float(running_loss) + float(loss.item()[0])\n",
    "            loss_hist.append(loss.item())\n",
    "            #print(t, loss.item())\n",
    "            loss_average = np.sum(loss_hist)/len(loss_hist)\n",
    "            loss_average_list.append(loss_average)\n",
    "            #print(loss_average)\n",
    "            print('Train Epoch: {0}, {1}'.format(epoch, loss_average))\n",
    "            \n",
    "for epoch in range(1,10):\n",
    "    train(epoch, sequence_list, ratings_list)\n",
    "    # NN speichern\n",
    "    torch.save(model,'LSTM.pt')\n",
    "    \n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1 = 128\n",
    "D_out = 1\n",
    "model = nn.DenseNet(n_users, n_items, n_factors=40,H1=H1,D_out=D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings = df_user_item_rating.pivot(index='reviewerID', columns='asin', values='overall').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(model, ratings, lr=1e-6):\n",
    "    loss_fn = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=lr)\n",
    "\n",
    "    # Sort our data\n",
    "    ratings = np.array(ratings)\n",
    "    users, items = ratings.nonzero()\n",
    "    t = 0.0\n",
    "    total_loss = torch.Tensor([0])\n",
    "    p = np.random.permutation(len(users))\n",
    "    users, items = users[p], items[p]\n",
    "    loss_hist = []\n",
    "    for user, item in zip(*(users, items)):\n",
    "        t = t + 1\n",
    "        # get user, item and rating data\n",
    "        rating = torch.FloatTensor([ratings[user, item]])\n",
    "        user = torch.LongTen\n",
    "        sor([int(user)])\n",
    "        item = torch.LongTensor([int(item)])\n",
    "    \n",
    "        # predict and calculate loss\n",
    "        prediction = model.forward(user, item).flatten()\n",
    "        loss = torch.sqrt(loss_fn(prediction, rating))\n",
    "    \n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Print\n",
    "        if t % 100 == 0:\n",
    "            #running_loss = float(running_loss) + float(loss.item()[0])\n",
    "            loss_hist.append(loss.item())\n",
    "            #print(t, loss.item())\n",
    "            print(np.sum(loss_hist)/len(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class Interactions(data.Dataset):\n",
    "    \"\"\"\n",
    "    Hold data in the form of an interactions matrix.\n",
    "    Typical use-case is like a ratings matrix:\n",
    "    - Users are the rows\n",
    "    - Items are the columns\n",
    "    - Elements of the matrix are the ratings given by a user for an item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mat):\n",
    "        self.mat = mat.astype(np.float32).tocoo()\n",
    "        self.n_users = self.mat.shape[0]\n",
    "        self.n_items = self.mat.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.mat.row[index]\n",
    "        col = self.mat.col[index]\n",
    "        val = self.mat.data[index]\n",
    "        return (row, col), val\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mat.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.array(ratings)\n",
    "for user in range(len(ratings)):\n",
    "        user_name = np.ones(ratings.shape[1])*user\n",
    "        user_name = user_name.astype(int)\n",
    "        j = ratings[user,:]\n",
    "        items = pd.DataFrame(ratings).columns.values\n",
    "        A = sparse.csr_matrix(((user_name),(items,j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings[:100000].to_csv('ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Hyperparameters\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "n_output = 1\n",
    "\n",
    "# add one to represent padding when there is not enough history\n",
    "model = LSTM.LSTMRating(embedding_dim, hidden_dim, n_items+1, n_output)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "#if os.path.isfile('LSTM.pt'):\n",
    "#    model = torch.load('LSTM.pt')\n",
    "\n",
    "# Trainingsprozess\n",
    "def train(epoch, ratings):\n",
    "    model.train()\n",
    "    loss_hist = []\n",
    "    loss_average_list = []\n",
    "    t = 0\n",
    "    ratings = ratings.astype(int)\n",
    "    ratings = np.array(ratings)\n",
    "    for user in range(len(ratings)):\n",
    "        t = t + 1\n",
    "        user_name = np.ones(ratings.shape[1])*user\n",
    "        user_name = user_name.astype(int)\n",
    "        j = ratings[user,:]\n",
    "        items = pd.DataFrame(ratings).columns.values\n",
    "        A = sparse.csr_matrix(((user_name),(items,j)))\n",
    "        print(A.tocoo().col)\n",
    "        X_col = A.tocoo().col\n",
    "        X_row = A.tocoo().row\n",
    "        indices = np.nonzero(X_col)\n",
    "        columns_non_unique = indices[0]\n",
    "        unique_columns = sorted(set(columns_non_unique))\n",
    "        user_item = A.tocoo().row[unique_columns]\n",
    "        user_rating = A.tocoo().col[unique_columns]\n",
    "        user = np.ones(len(user_item))*5\n",
    "        user = user.astype(int)\n",
    "        user_item_rating = sparse.csr_matrix(((user),(user_item,user_rating)))\n",
    "        #print(user_item_rating)\n",
    "        user_item_rating.tocoo().row\n",
    "        \n",
    "        sequence = user_item_rating.tocoo().row\n",
    "        target_ratings = user_item_rating.tocoo().col\n",
    "        #print(sequence)\n",
    "        #print(target_ratings)\n",
    "        # Set gardient zero\n",
    "        model.zero_grad()\n",
    "        # initialize hidden layers\n",
    "        model.hidden = model.init_hidden()\n",
    "        # convert sequence to PyTorch variables\n",
    "        sequence_var = torch.LongTensor(sequence.astype('int64'))\n",
    "        #print(sequence_var)\n",
    "        # forward pass\n",
    "        ratings_scores = model(sequence_var).flatten()\n",
    "        target_ratings_var = torch.FloatTensor(target_ratings.astype('float32')).flatten()\n",
    "        # compute loss\n",
    "        loss = torch.sqrt(loss_fn(ratings_scores, target_ratings_var))\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # print\n",
    "        if t % 100 == 0:\n",
    "            #running_loss = float(running_loss) + float(loss.item()[0])\n",
    "            loss_hist.append(loss.item())\n",
    "            #print(t, loss.item())\n",
    "            loss_average = np.sum(loss_hist)/len(loss_hist)\n",
    "            loss_average_list.append(loss_average)\n",
    "            #print(loss_average)\n",
    "            print('Train Epoch: {0}, {1}'.format(epoch, loss_average))\n",
    "        \n",
    "for epoch in range(1,3):\n",
    "    train(epoch, interactions)\n",
    "        \n",
    "# NN speichern\n",
    "torch.save(model,'LSTM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_average_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(preds, vals):\n",
    "    sig = nn.Sigmoid()\n",
    "    return (1.0 - sig(preds)).pow(2).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
