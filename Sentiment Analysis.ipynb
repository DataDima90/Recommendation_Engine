{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of notebooks we are building different machine learning models to detect sentiment (i.e. detect if a review is positive or negative) using Pytorch.\n",
    "\n",
    "Sentiment analysis make sure that we are truly knowing to what our customers think, want and need.\n",
    "\n",
    "Here are some benefits of sentiment analysis:\n",
    "- Adjust marketing strategy: How we can know if we are doing the right thinks in e.g. social media etc.? The information in e.g. social media etc. provide us knowing what our customers feel and think about our brand.\n",
    "- Measure ROI of your marketing campaign: Success of marketing campaign can lies in how positive discussions are amongst the customers.\n",
    "- Develop a better product: Sentiment analysis helps us complete our market research by getting to know what our customers' opinions are about our product/services and how we can align our products/services quality and features with their tastes.\n",
    "- Improve a better customer service: Sentiment analysis can pick up negative discussions, and give us real-time alerts so that we can respond quickly. Sentiment analysis as part of social listening to manage complaints can help us avoid leaving our customers feeling ignored and angry.\n",
    "- Crisis management: Constant monitoring of what is currently happening in social media conversations also helps us to prevent or at least mitigate the damage of online communication crisis.\n",
    "\n",
    "\n",
    "This will be done on Amazon reviews, using the ... dataset.\n",
    "In this first notebook, we'll start very simple to understand the general concepts. Futher notebooks will build on this knowledge and we will actually get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we show how to approach a sentiment analysis problem. We will be starting with preprocessing and exploration of data. Then we extracted features from the cleaned text using Bag-of-Words, TF-IDF and Word_Embedding (Word2Vec). Finally, we were able to build a couple of models using the feature sets to classify the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dimitriwilhelm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import datapreprocessing as dp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bag-of-Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly start and read the file from the dataset in order to perform different tasks on it. We will use the Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data provided via http://jmcauley.ucsd.edu/data/amazon/\n",
    "def parse(path):\n",
    "    g = open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Apps = getDF(\"data/Apps_for_Android_5.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new dataframe with the columns 'reviewText' and 'sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column called 'sentiment'\n",
    "df_Apps['sentiment'] = [1 if x > 3 else 0 for x in df_Apps['overall']]\n",
    "# New DataFrame with columsn 'reviewText' and 'sentiment'\n",
    "df_sentiment_data = df_Apps[['reviewText', 'sentiment']]\n",
    "\n",
    "# Rename our columns\n",
    "#review_data = df_sentiment_data.reviewText\n",
    "#labels = df_sentiment_data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 0\n",
      "Maximum review lenght: 18077\n"
     ]
    }
   ],
   "source": [
    "print(f'Minimum review length: {len(min(df_sentiment_data.reviewText, key=len))}')\n",
    "print(f'Maximum review lenght: {len(max(df_sentiment_data.reviewText, key=len))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we will discuss different feature extraction methods, starting with some basic techniques and learn about preprocessing of the text data in order to extract better features from clean data, which will lead into further Natural Language Processing techniques at the end.\n",
    "\n",
    "1. Feature extraction using text data\n",
    "2. Data Preprocessing of text data\n",
    "3. Further Textprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction using text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract some feature of our data\n",
    "def extract_data(data):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(data)\n",
    "    # Number of words in each review\n",
    "    # Intuition: Negative sentiments conatin a lesser amout of words than the positive ones\n",
    "    data['word_count'] = data['reviewText'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    \n",
    "    # Number of characters in each review\n",
    "    data['char_count'] = data['reviewText'].str.len() ## this includes spaces too\n",
    "    \n",
    "    # Number of stopwords\n",
    "    # While solving a NLP problem, the first thing we do is to remove the stopwords.\n",
    "    data['numb_stopwords'] = data['reviewText'].apply(\n",
    "        lambda x: len([x for x in x.split() if x in stop])\n",
    "    )\n",
    "    \n",
    "    # Number of special characters\n",
    "    \n",
    "    \n",
    "    # Number of numerics\n",
    "    # It could be a useful feature that should be run\n",
    "    data['numerics'] = data['reviewText'].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isdigit()])\n",
    "    )\n",
    "    \n",
    "    # Number of Uppercase words\n",
    "    # Anger or rage is quite often expressed by writing in UPPERCASE words\n",
    "    data['uppercase'] = data['reviewText'].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isupper()])\n",
    "    )\n",
    "    \n",
    "    # Check of empty reviews\n",
    "    data['avg_word'] = data['reviewText'].apply(\n",
    "        lambda x: sum( len(word) for word in (x.split()) )\n",
    "    )\n",
    "    idx = data[data.avg_word == 0].index\n",
    "    data = data.drop(idx)\n",
    "    \n",
    "    # Average Word Length\n",
    "    # We simply take the sum of the length of all the words and divide it by the total length of the review\n",
    "    data['avg_word'] = data['reviewText'].apply(\n",
    "        lambda x: sum( len(word) for word in (x.split())) / (len(x.split()) )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(df_sentiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numb_stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>uppercase</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loves the song, so he really couldn't wait to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>206</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh, how my little grandson loves this app. He'...</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>255</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I found this at a perfect time since my daught...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>288</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My 1 year old goes back to this game over and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>186</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are three different versions of the song...</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>746</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  sentiment  word_count  \\\n",
       "0  Loves the song, so he really couldn't wait to ...          0          41   \n",
       "1  Oh, how my little grandson loves this app. He'...          1          47   \n",
       "2  I found this at a perfect time since my daught...          1          53   \n",
       "3  My 1 year old goes back to this game over and ...          1          43   \n",
       "4  There are three different versions of the song...          1         134   \n",
       "\n",
       "   char_count  numb_stopwords  numerics  uppercase  avg_word  \n",
       "0         206              19         1          1       166  \n",
       "1         255              21         0          0       209  \n",
       "2         288              19         0          1       236  \n",
       "3         186              15         2          0       144  \n",
       "4         746              55         0          0       613  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the first 5 rows\n",
    "df_sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract some features from text data. Before we diving into text and feature extraction, our next step will be clean the data in order to obtain better features. We achieve this by doing some of the preprocessing steps on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing of text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing and cleaning is an important step before any text mining task. In this step we will remove the punctuations, stopwords and normalize the reviews as much as possible.\n",
    "\n",
    "All these data preprocessing steps are essential and will help us in reducing our vocabulary clutter so that the features produced in the end are more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean our data\n",
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Make entire text lowercase\n",
    "    # Transform our review into lower case. This avoids having multiple copies of the same words\n",
    "    data['reviewText'] = data['reviewText'].apply(\n",
    "        lambda x: \" \".join( x.lower() for x in x.split() )\n",
    "    )\n",
    "    \n",
    "    # Removing Punctuation, Numbers and Special Characters/Symbols\n",
    "    # It does not add any extra information while treating text data. Therefore it will help us reduce the size of the data\n",
    "    data['reviewText'] = data['reviewText'].str.replace('[^a-zA-Z#]',' ')\n",
    "    \n",
    "    # Removal of Stop Words, i.e. we just removed commonly occurring words in a general sense\n",
    "    # Stop Words should be removed from the text data. We use for this predefined libraries from nltk\n",
    "    data['reviewText'] = data['reviewText'].apply(\n",
    "        lambda x: \" \".join( x for x in x.split() if x not in stop)\n",
    "    )\n",
    "    \n",
    "    # Removing commonly occurring words from our text data\n",
    "    # Let's check the 10 most frequently occuring words in our text data\n",
    "    freq = pd.Series(\" \".join( data['reviewText'] ).split()).value_counts()[:1]\n",
    "    # Let's remove these words as their presence will not of any use in classification of our text data\n",
    "    freq = list(freq.index)\n",
    "    data['reviewText'] = data['reviewText'].apply(\n",
    "        lambda x: \" \".join( x for x in x.split() if x not in freq)\n",
    "    )\n",
    "    \n",
    "    # Remove rare words\n",
    "    # Let's check the 10 rarely occurring words in our text data\n",
    "    #freq_1 = pd.Series(\" \".join( data['reviewText'] ).split()).value_counts()[:-1]\n",
    "    # Let's remove these words as their presence will not of any use in classification of our text data\n",
    "    #freq_1 = list(freq.index)\n",
    "    #data['reviewText'] = data['reviewText'].apply(\n",
    "    #    lambda x: \" \".join(x for x in x.split() if x not in freq_1)\n",
    "    #)\n",
    "    \n",
    "    # Stemming, i.e. we're removing suffices, like \"ing\", \"ly\", etc. by a simple rule-based approach.\n",
    "    # For this purpose, we will use PorterStemmer from the NLTK library\n",
    "    #st = PorterStemmer()\n",
    "    #data['reviewText'] = data['reviewText'].apply(\n",
    "    #    lambda x: \" \".join([ st.stem(word) for word in x.split() ])\n",
    "    #)\n",
    "   \n",
    "    # Lemmatization\n",
    "    # Lemmatization is more effective that stemming because it converts the word into its root word, \n",
    "    # rather than just stripping the suffices. We usually prefer using lemmatiziation over stemming.\n",
    "    data['reviewText'] = data['reviewText'].apply(\n",
    "        lambda x: \" \".join([ Word(word).lemmatize() for word in x.split() ])\n",
    "    )\n",
    "    \n",
    "    # Remove short words (Length < 3)\n",
    "    data['reviewText'] = data['reviewText'].apply(\n",
    "        lambda x: \" \".join([w for w in x.split() if len(w) > 3])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df_sentiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numb_stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>uppercase</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love song really wait play little interesting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>206</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>little grandson love always asking monkey gran...</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>255</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>found perfect time since daughter favorite son...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>288</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>year back simple easy toddler even caught year...</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>186</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>three different version song keep occupied eve...</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>746</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  sentiment  word_count  \\\n",
       "0  love song really wait play little interesting ...          0          41   \n",
       "1  little grandson love always asking monkey gran...          1          47   \n",
       "2  found perfect time since daughter favorite son...          1          53   \n",
       "3  year back simple easy toddler even caught year...          1          43   \n",
       "4  three different version song keep occupied eve...          1         134   \n",
       "\n",
       "   char_count  numb_stopwords  numerics  uppercase  avg_word  \n",
       "0         206              19         1          1       166  \n",
       "1         255              21         0          0       209  \n",
       "2         288              19         0          1       236  \n",
       "3         186              15         2          0       144  \n",
       "4         746              55         0          0       613  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment_data = data\n",
    "df_sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After every preprocessing step, it is a good practice to check the most frequent words in the data. Therefore we define a function that would plot a bar graph of n most frequent words in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have done all the preprocessing steps in order to clean our text data. Now, we can finally move on to extracting features using NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and  visualizing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the cleaned reviews.\n",
    "\n",
    "Before we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:\n",
    "\n",
    "- What are the most common words in the entire dataset?\n",
    "- What are the most common words in the dataset for negative and positive reviews, respectively?\n",
    "- How many  are there in a review?\n",
    "- Which trends are associated with my dataset?\n",
    "- Which trends are associated with either of the sentiments? Are they compatible with the sentiments?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the common words in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the given sentiments are distributed across the review dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.\n",
    "\n",
    "At first we define our sets of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(data):\n",
    "    return ' '.join([word for word in data['reviewText']])\n",
    "\n",
    "def negative_sentiment_words(data):\n",
    "    return ' '.join([text for text in data['reviewText'][data['sentiment'] == 0]])\n",
    "\n",
    "def positive_sentiment_words(data):\n",
    "    return ' '.join([text for text in data['reviewText'][data['sentiment'] == 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s visualize all the words of our data using the wordcloud plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(data):\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(data)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All words in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words in our dataset\n",
    "all_words = total_words(data)\n",
    "\n",
    "# Plotting our whole words using the wordcloud plot\n",
    "plot_wordcloud(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the words are positive or neutral. With kindle and fire being the most frequent ones. Next we will plot separate wordclouds for both the classes (negative or positive) in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all words in our positive sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words in our positive sentiment dataset\n",
    "positive_words = positive_sentiment_words(data)\n",
    "\n",
    "# Plotting our positive words using the wordcloud plot\n",
    "plot_wordcloud(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the words are positive or neutral. With kindle and fire being the most frequent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all words in our negative sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words in our negative sentiment dataset\n",
    "negative_words = negative_sentiment_words(data)\n",
    "\n",
    "# Plotting our negative words using the wordcloud plot\n",
    "plot_wordcloud(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the words are positive or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start using logistic regression to build the models. It predicts the probability of occurrence of an event by fitting data to a logit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our input data and target\n",
    "X = df_sentiment_data['reviewText']\n",
    "y = df_sentiment_data['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3) \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, random_state=1, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train_data set size: {}'.format(X_train.shape[0]))\n",
    "print('Train_target set size: {}'.format(y_train.shape[0]))\n",
    "print('Val_data set size: {}'.format(X_val.shape[0]))\n",
    "print('Val_target set size: {}'.format(y_val.shape[0]))\n",
    "print('Test_data set size: {}'.format(X_test.shape[0]))\n",
    "print('Test_target set size: {}'.format(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from our cleaned text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques \n",
    "\n",
    "- Bag-of-Words\n",
    "- TF-IDF \n",
    "- Word Embeddings\n",
    "\n",
    "In this notebook, we will be covering all of them. We will start with Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model using Bag-of-Words features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words is a method to represent text into numerical features. Consider a corpus (a collection of texts) called C of D documents {d_1,d_2…..d_D} and N unique tokens extracted out of the corpus C. The N tokens (words) will form a list, and the size of the bag-of-words matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).\n",
    "\n",
    "Bag-of-Words features can be easily created using sklearn’s CountVectorizer function. We will set the parameter max_features = 1000 to select only top 1000 terms ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a CountVectorizer object: CountVectorizer()\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english', ngram_range=(1,1))\n",
    "\n",
    "# fit and transforms the train data into a bag of words feature matrix\n",
    "count_train = bow_vectorizer.fit(X_train)\n",
    "X_train_bow = bow_vectorizer.transform(X_train)\n",
    "\n",
    "# transform the test data\n",
    "X_val_bow = bow_vectorizer.transform(X_val)\n",
    "\n",
    "# transform the text for submission at the end\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vec\n",
    "print(\"Every feature:\\n{}\".format(bow_vectorizer.get_feature_names()))\n",
    "print(\"\\nEvery 3rd feature:\\n{}\".format(bow_vectorizer.get_feature_names()[::3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary and vocabulary ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the columns in the above matrix can be used as features to build a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build our functions for the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression(X_train, y_train):\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    return log_reg\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, threshold=0.5):\n",
    "    cv_loss = np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='neg_log_loss'))\n",
    "    print('CV Log_loss score is {}'.format(cv_loss))\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score is {}'.format(cv_score))\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_prob = model.predict_proba(X_val) # predicting on the validation set\n",
    "    y_pred_prob_int = y_pred_prob[:,1] >= threshold # if prediction is greater than or equal to 0.5 than 1 else 0\n",
    "    y_pred_prob_int = y_pred_prob_int.astype(np.int)\n",
    "\n",
    "    # calculate the auc-score\n",
    "    auc_score = metrics.roc_auc_score(y_val, y_pred_prob_int)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "\n",
    "    target_name = ['Negative', 'Positive']\n",
    "    print(classification_report(y_val, y_pred, target_names=target_name))\n",
    "    \n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    #cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm = pd.DataFrame(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion_Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_name))\n",
    "    plt.xticks(tick_marks, target_name, rotation=45)\n",
    "    plt.yticks(tick_marks, target_name)\n",
    "\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(i, j, cm.iloc[i, j],\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def predict(model, X_test, threshold=0.5):\n",
    "    # predicting on the test set\n",
    "    test_pred = model.predict_proba(X_test)\n",
    "\n",
    "    # if prediction is greater than or equal to 0.5 than 1 else 0\n",
    "    test_pred_int = test_pred[:,1] >= threshold\n",
    "    test_pred_int = test_pred_int.astype(np.int)\n",
    "    return test_pred_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_bow = run_logistic_regression(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "evaluate_model(model_bow, X_train_bow, X_val_bow, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the logistic regression model on the Bag-of-Words features. Now we will use this model to predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_bow = predict(model_bow, X_test_bow)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test['pred_bow_sentiment'] = pred_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our result \n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will again train a logistic regression model but this time on the TF-IDF features. Let’s see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model using TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account, not just the occurrence of a word in a single document (or reviews) but in the entire corpus.\n",
    "\n",
    "TF-IDF stands for \"Term Frequency, Inverse Document Frequency\". It is a way to score the importance of words in a document based on how frequently they appear across multiple documents.\n",
    "\n",
    "Intuively:\n",
    "- If a word appears frequently in a document, it's important. Give the word a high score.\n",
    "- If a word appears in many documents, it's not a unique identifier. Give the word a low score.\n",
    "\n",
    "That's why, common words like \"the\" and \"for\", which appear in many documents, will be scaled down. Words that appear frequently in a single document will be scaled up.\n",
    "\n",
    "Let’s have a look at the important terms related to TF-IDF:\n",
    "\n",
    "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "- IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "- TF-IDF = TF*IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF - The Term Trequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer object: TfidfVectorizer()\n",
    "tfidf_vectorizer= TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "\n",
    "# fit and transforms the train data into a tf-idf feature matrix\n",
    "tfidf_fitted = tfidf_vectorizer.fit(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "# transform the val data\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# transform the text\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_tfidf = run_logistic_regression(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "evaluate_model(model_tfidf, X_train_tfidf, X_val_tfidf, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_tfidf = predict(model_tfidf, X_test_tfidf)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test['pred_tfidf_sentiment'] = pred_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The learned corpus vocabulary: \\n {tfidf_vectorizer.vocabulary_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF: The inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = tfidf_vectorizer.idf_\n",
    "idf_dict = tfidf_fitted.get_feature_names()\n",
    "print(f'The learning TF-IDF: \\n {(dict(zip(idf_dict, idf)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = dict(zip(idf_dict[:10], idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dict_freq(data):\n",
    "    token_weight = pd.DataFrame.from_dict(data, orient='index').reset_index()\n",
    "    token_weight.columns=('token','weight')\n",
    "    token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
    "\n",
    "    sns.barplot(x='token', y='weight', data=token_weight)            \n",
    "    #plt.title(\"Inverse Document Frequency(idf) per token\")\n",
    "    fig=plt.gcf()\n",
    "    fig.set_size_inches(18,10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_dict_freq(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use advanced techniques like word2vec model for feature extraction and neural networks (LSTM).\n",
    "\n",
    "For that we wil use Word Embeddings (word2vec and doc2vec) for creating better features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams (sets of consecutive words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "training_data = []\n",
    "for i in range(len(review_data[:10])):\n",
    "    training_data.append(\n",
    "        (review_data[i].split(), [labels[i]])\n",
    "    )\n",
    "    \n",
    "# First, we build an index of all tokens in the data.\n",
    "token_index={}\n",
    "for sample in review_data[:10]:\n",
    "    #print(sample.split())\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index)\n",
    "print(training_data)\n",
    "#print(token_index)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an LSTM model for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with a standard RNN\n",
    "\n",
    "The simplest RNN model has a major drawback, called vanishing gradient problem, which prevents it from being accurate. The problem comes from the fact that at each time step during training we are using the same weights to calculate y_t. That multiplication is also done during backpropagation. The further we move backwards, the bigger or smaller our error signal becommes. This means that the network experiences difficulty in memorizing words from far away in the sequence and makes predictions based onv only the most recent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch’s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The three dimensions of this input are:\n",
    "\n",
    "- Samples: One sequence is one sample. A batch is comprised of one or more samples.\n",
    "- Time Steps: One time step is one point of observation in the sample.\n",
    "- Features: One feature is one observation at a time step.\n",
    "\n",
    "This means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature.\n",
    "\n",
    "When defining the input layer of your LSTM network, the network assumes you have 1 or more samples and requires that you specify the number of time steps and the number of features. You can do this by specifying a tuple to the “input_shape” argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start building our model architecture in the code below.\n",
    "\n",
    "Input: Our input is a sequence of words (technically, integer wordIDs) of maximum length = max_words\n",
    "Output: Our Output is a binary sentiment label(0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, sentence_dim, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sentence_dim = sentence_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # The linear layer that maps from hidden state space to label_size\n",
    "        self.linear = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (torch.zeros(self.batch_size, self.sentence_dim, self.hidden_dim),\n",
    "                torch.zeros(self.batch_size, self.sentence_dim, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        x = embedded.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.linear(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # These will usually be more like 32 or 64 dimensional.\n",
    "    # We will keep them small, so we can see how the weights change as we train.\n",
    "    EMBEDDING_DIM = 6\n",
    "    HIDDEN_DIM = 6\n",
    "    INPUT_SIZE = len(token_index)\n",
    "    OUTPUT_DIM = 2\n",
    "    EPOCH = 20\n",
    "    \n",
    "    # Train the model\n",
    "    model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, INPUT_SIZE, OUTPUT_DIM, SENTENCE_DIMA, BATCH_SIZE)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1e-3)\n",
    "    \n",
    "    # See what the scores are before training\n",
    "    # Note that element i,j of the output is the score for tag j for word i.\n",
    "    # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "    #with torch.no_grad():\n",
    "    #    inputs = prepare_sequence(training_data[1][0], token_index)\n",
    "    #    sentiment_scores = model(inputs)\n",
    "\n",
    "    for epoch in range(EPOCH):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        epoch_loss = 0\n",
    "        count = 0\n",
    "        for sentence, sentiment in training_data:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, token_index)\n",
    "\n",
    "            targets = torch.tensor([1], dtype=torch.long)\n",
    "            # Step 3. Run our forward pass.\n",
    "            sentiment_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(sentiment_scores, targets)\n",
    "            count += 1\n",
    "            if count % 500 == 0:\n",
    "                print(f'epoch: {epoch} iterations: {count} loss {loss.item()}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                   \n",
    "            epoch_loss += loss.item()\n",
    "            #print(epoch_loss)\n",
    "        #print(f'{epoch_loss}')\n",
    "        print(f'Epoche {epoch}: {epoch_loss / (len(training_data))}')\n",
    "        \n",
    "train()\n",
    "# See what the scores are after training\n",
    "#with torch.no_grad():\n",
    "#    inputs = prepare_sequence(training_data[1][0], token_index)\n",
    "#    sentiment_scores = model(inputs)\n",
    "    #acc = get_accuracy(sentiment_scores, [1])\n",
    "#    print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. \n",
    "\n",
    "Why do we need Word Embeddings?\n",
    "\n",
    "Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw text. They requires numbers as inputs to perform any sort of job.\n",
    "\n",
    "A Word Embedding format generally tries to map a word using a dictionary to a vector.\n",
    "\n",
    "Example sentence: \"I love programming\"\n",
    "\n",
    "A word in this sentence may be \"love\" or \"programming\" etc.\n",
    "\n",
    "A dictionary may be the list of all unique words in the sentence. So, a dictionary may look like - [\"I\", \"love\", \"programming\"]\n",
    "\n",
    "A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of \"love\" in this format according to the above dictionary is [0,1,0] and of \"programming is [0,0,1].\n",
    "\n",
    "This is a very simple method to represent a word in the vector form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Word2Vec is pretty simple. If you have two words that have very similar neighbors, i.e. the context in which its used is about the same, then then these words are probably quite similar in meaning or are at least related. For example, the words shocked, appalled and astonished are usually used in a similar context.\n",
    "\n",
    "Using this underlying assumption, you can use Word2Vec to surface similar concepts, find unrelated concepts, compute similarity between two words and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Word2Vec model\n",
    "\n",
    "Our parameters in order to train our model:\n",
    "- size: The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n",
    "- window: The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "- min_count: Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "- workers: How many threads to use behind the scenes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we create a new list of lists of our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "for i in range(len(df_sentiment_data[:20000])):\n",
    "    #list1 = sent_data.reviewText[:10].apply(lambda x: word_tokenize(x))\n",
    "    review_list.append(df_sentiment_data.reviewText[:20000][i].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = Word2Vec(review_list, size=50, min_count=1, window=5, workers=5)\n",
    "model.train(review_list, total_examples=len(review_list), epochs=10)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(f'Our model is trained: {model}')\n",
    "\n",
    "# summarize vocabulary\n",
    "#words = model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "#print (new_model)\n",
    "\n",
    "# That looks pretty good, right? Let's look at a few more. Let's look at similarity for polite, france and shocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how good is our model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access vector for one word\n",
    "w1 = 'tthis'\n",
    "print(model.wv.most_similar(positive=w1, topn=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we build an index of all tokens in the data for our Word2Vec.\n",
    "def token_ix(model, data):\n",
    "    token_index={}\n",
    "    for sample in data:\n",
    "        for word in sample.split():\n",
    "            if word in model.wv:\n",
    "                token_index[word] = model.wv[word]\n",
    "    return token_index\n",
    "\n",
    "token_index = token_ix(model, df_sentiment_data['reviewText'][:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(data_review, data_sentiment, num_words, token_index):\n",
    "    tt = torch.zeros((len(data_review), num_words, len(token_index['love'])))\n",
    "    target = torch.zeros(len(data_review), dtype=torch.long)\n",
    "    #data_review = data_review.apply(lambda x: x.split()[:num_words])\n",
    "    data_review = data_review.apply(lambda x: x if len(x.split()) > num_words else 0)\n",
    "    #print(len(data_review[0]))\n",
    "    pad_seq_list = []\n",
    "    for i in range(len(data_review)):\n",
    "        pad_seq_list.append(\n",
    "            (data_review[i], [data_sentiment[i]])\n",
    "        )\n",
    "    for i in range(len(data_review)):\n",
    "        idx = [token_index[word] for word in pad_seq_list[i][0]]\n",
    "        target[i] = int(data_sentiment[i])\n",
    "        tt[i] = torch.tensor(idx, dtype=torch.float)\n",
    "    \n",
    "    tt = tt.transpose(1,0)\n",
    "    return tt, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = X\n",
    "#xy = xy.apply(lambda x: x.split())\n",
    "xy = xy.apply(lambda x: x if len(x.split()) > 10 else 0)\n",
    "non_zero = xy[xy == 0].index\n",
    "#len(xy)\n",
    "#X = X[non_zero]\n",
    "#y = y[non_zero]\n",
    "X_filter = X.drop(non_zero)\n",
    "y_filter = y.drop(non_zero)\n",
    "X_new = X_filter.reset_index(drop=True, inplace=True)\n",
    "y_new = y_filter.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_filter[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and test set\n",
    "X_train = X_filter[:20000]\n",
    "y_train = y_filter[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target_data = pad_seq(X_train, y_train, 5, token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train = input_data[:, :8000]\n",
    "input_data_val = input_data[:, 8001:9000]\n",
    "target_train = target_data[:8000]\n",
    "target_val = target_data[8001:9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, sentence_dim, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sentence_dim = sentence_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # The linear layer that maps from hidden state space to label_size\n",
    "        self.linear = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (torch.zeros(self.batch_size, self.sentence_dim, self.hidden_dim),\n",
    "                torch.zeros(self.batch_size, self.sentence_dim, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        lstm_out, self.hidden = self.lstm(sentence)\n",
    "        y  = self.linear(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train for Word2Vec\n",
    "def train():\n",
    "    # These will usually be more like 32 or 64 dimensional.\n",
    "    # We will keep them small, so we can see how the weights change as we train.\n",
    "    EMBEDDING_DIM = 50\n",
    "    HIDDEN_DIM = 6\n",
    "    INPUT_SIZE = len(token_index)\n",
    "    OUTPUT_DIM = 2\n",
    "    SENTENCE_DIM = 3\n",
    "    BATCH_SIZE = 2\n",
    "    EPOCH = 300\n",
    "    SIZE = 50\n",
    "    \n",
    "    mean_val_loss = []\n",
    "    mean_train_loss = []\n",
    "    # Train the model\n",
    "    model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, INPUT_SIZE, OUTPUT_DIM, SENTENCE_DIM, BATCH_SIZE)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1e-2)\n",
    "    \n",
    "    # See what the scores are before training\n",
    "    # Note that element i,j of the output is the score for tag j for word i.\n",
    "    # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "    #with torch.no_grad():\n",
    "    #    inputs = prepare_sequence(training_data[1][0], token_index)\n",
    "    #    sentiment_scores = model(inputs)\n",
    "    \n",
    "    for epoch in range(EPOCH):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        #model = torch.load('lstm.pt')\n",
    "        epoch_loss = 0\n",
    "        count = 0\n",
    "        val_loss = []\n",
    "        train_loss = []\n",
    "        sample_count = len(input_data_train)\n",
    "        sample_range = int(sample_count/BATCH_SIZE)\n",
    "        for sample in range(0,sample_range):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.  \n",
    "            # create mini_batch\n",
    "            input_batch = input_data_train[:, sample * BATCH_SIZE : (sample + 1) * BATCH_SIZE]\n",
    "            target_batch = target_train[sample * BATCH_SIZE : (sample + 1) * BATCH_SIZE]\n",
    "    \n",
    "            # Step 3. Run our forward pass.\n",
    "            sentiment_scores = model(input_batch)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(sentiment_scores, target_batch)\n",
    "            #count += 1\n",
    "            #if count % 500 == 0:\n",
    "                #print(f'epoch: {epoch} iterations: {count} loss {loss.item()}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                   \n",
    "            #epoch_loss += loss.item()\n",
    "            train_loss.append(loss.data)\n",
    "        #torch.save(model, 'lstm.pt')\n",
    "        #print(f'{epoch}: {epoch_loss}')\n",
    "    \n",
    "        # Each Epoch we validate our model\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                sample_count = len(input_data_val)\n",
    "                sample_range = int(sample_count/BATCH_SIZE)\n",
    "                for sample in range(0, sample_range):\n",
    "                    input_batch = input_data_val[:, sample * BATCH_SIZE : (sample + 1) * BATCH_SIZE]\n",
    "                    target_batch = target_val[sample * BATCH_SIZE : (sample + 1) * BATCH_SIZE]\n",
    "                    sentiment_scores = model(input_batch)\n",
    "                    loss = loss_function(sentiment_scores, target_batch)\n",
    "                    val_loss.append(loss.data)\n",
    "                \n",
    "                    _, predicted = torch.max(sentiment_scores.data, 1)\n",
    "                    correct += (predicted == target_batch).sum().item()\n",
    "                    total += target_batch.size(0)\n",
    "            \n",
    "            mean_train_loss.append(np.mean(train_loss))\n",
    "            mean_val_loss.append(np.mean(val_loss))\n",
    "        \n",
    "        \n",
    "            print('Epoch:[{}/{}], train loss : {:.4f}, val loss : {:.4f}, val acc : {:.2f}%'\\\n",
    "              .format(epoch+1, EPOCH, np.mean(train_loss),\\\n",
    "                               np.mean(val_loss), 100*correct/total))\n",
    "        \n",
    "    # Plotting our test and val losses\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.plot(mean_train_loss, label='train')\n",
    "    ax.plot(mean_val_loss, label='val')\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(lines, labels, loc='best')\n",
    "    plt.ylim(0,0.90)\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "train()\n",
    "# See what the scores are after training\n",
    "#with torch.no_grad():\n",
    "#    inputs = prepare_sequence(training_data[1][0], token_index)\n",
    "#    sentiment_scores = model(inputs)\n",
    "    #acc = get_accuracy(sentiment_scores, [1])\n",
    "#    print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
